<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>prompt Injection</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.10/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.18.10/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.10/dist/index.js"></script><script>(()=>{setTimeout(()=>{const{markmap:x,mm:K}=window,P=new x.Toolbar;P.attach(K);const F=P.render();F.setAttribute("style","position:absolute;bottom:20px;right:20px"),document.body.append(F)})})()</script><script>((b,L,T,D)=>{const H=b();window.mm=H.Markmap.create("svg#mindmap",(L||H.deriveOptions)(D),T)})(()=>window.markmap,null,{"content":"LLM01 : Prompt Injection","children":[{"content":"Scenario #1: Direct Injection","children":[{"content":"An attacker injects a prompt into a customer support chatbot","children":[],"payload":{"tag":"li","lines":"9,10"}},{"content":"Instructs it to ignore previous guidelines","children":[],"payload":{"tag":"li","lines":"10,11"}},{"content":"Queries private data stores","children":[],"payload":{"tag":"li","lines":"11,12"}},{"content":"Sends emails","children":[],"payload":{"tag":"li","lines":"12,13"}},{"content":"Leads to unauthorized access and privilege escalation","children":[],"payload":{"tag":"li","lines":"13,15"}}],"payload":{"tag":"h3","lines":"8,9"}},{"content":"Scenario #2: Indirect Injection","children":[{"content":"A user employs an LLM to summarize a webpage with hidden instructions","children":[],"payload":{"tag":"li","lines":"16,17"}},{"content":"Causes the LLM to insert an image linking to a URL","children":[],"payload":{"tag":"li","lines":"17,18"}},{"content":"Leads to exfiltration of the private conversation","children":[],"payload":{"tag":"li","lines":"18,20"}}],"payload":{"tag":"h3","lines":"15,16"}},{"content":"Scenario #3: Unintentional Injection","children":[{"content":"A company includes an instruction in a job description to identify AI-generated applications","children":[],"payload":{"tag":"li","lines":"21,22"}},{"content":"An applicant uses an LLM to optimize their resume","children":[],"payload":{"tag":"li","lines":"22,23"}},{"content":"Inadvertently triggers the AI detection","children":[],"payload":{"tag":"li","lines":"23,25"}}],"payload":{"tag":"h3","lines":"20,21"}},{"content":"Scenario #4: Intentional Model Influence","children":[{"content":"An attacker modifies a document in a repository used by a RAG application","children":[],"payload":{"tag":"li","lines":"26,27"}},{"content":"User&apos;s query returns the modified content","children":[],"payload":{"tag":"li","lines":"27,28"}},{"content":"Malicious instructions alter the LLM&apos;s output","children":[],"payload":{"tag":"li","lines":"28,29"}},{"content":"Generates misleading results","children":[],"payload":{"tag":"li","lines":"29,31"}}],"payload":{"tag":"h3","lines":"25,26"}},{"content":"Scenario #5: Code Injection","children":[{"content":"An attacker exploits a vulnerability (CVE-2024-5184) in an LLM-powered email assistant","children":[],"payload":{"tag":"li","lines":"32,33"}},{"content":"Injects malicious prompts","children":[],"payload":{"tag":"li","lines":"33,34"}},{"content":"Allows access to sensitive information","children":[],"payload":{"tag":"li","lines":"34,35"}},{"content":"Manipulates email content","children":[],"payload":{"tag":"li","lines":"35,37"}}],"payload":{"tag":"h3","lines":"31,32"}},{"content":"Scenario #6: Payload Splitting","children":[{"content":"An attacker uploads a resume with split malicious prompts","children":[],"payload":{"tag":"li","lines":"38,39"}},{"content":"LLM evaluates the candidate","children":[],"payload":{"tag":"li","lines":"39,40"}},{"content":"Combined prompts manipulate the model&apos;s response","children":[],"payload":{"tag":"li","lines":"40,41"}},{"content":"Results in a positive recommendation despite the actual resume contents","children":[],"payload":{"tag":"li","lines":"41,43"}}],"payload":{"tag":"h3","lines":"37,38"}},{"content":"Scenario #7: Multimodal Injection","children":[{"content":"An attacker embeds a malicious prompt within an image accompanying benign text","children":[],"payload":{"tag":"li","lines":"44,45"}},{"content":"Multimodal AI processes the image and text concurrently","children":[],"payload":{"tag":"li","lines":"45,46"}},{"content":"Hidden prompt alters the model&apos;s behavior","children":[],"payload":{"tag":"li","lines":"46,47"}},{"content":"Leads to unauthorized actions or disclosure of sensitive information","children":[],"payload":{"tag":"li","lines":"47,49"}}],"payload":{"tag":"h3","lines":"43,44"}},{"content":"Scenario #8: Adversarial Suffix","children":[{"content":"An attacker appends a seemingly meaningless string of characters to a prompt","children":[],"payload":{"tag":"li","lines":"50,51"}},{"content":"Influences the LLM&apos;s output in a malicious way","children":[],"payload":{"tag":"li","lines":"51,52"}},{"content":"Bypasses safety measures","children":[],"payload":{"tag":"li","lines":"52,54"}}],"payload":{"tag":"h3","lines":"49,50"}},{"content":"Scenario #9: Multilingual/Obfuscated Attack","children":[{"content":"An attacker uses multiple languages or encodes malicious instructions (e.g., using Base64 or emojis)","children":[],"payload":{"tag":"li","lines":"55,56"}},{"content":"Evades filters","children":[],"payload":{"tag":"li","lines":"56,57"}},{"content":"Manipulates the LLM&apos;s behavior","children":[],"payload":{"tag":"li","lines":"57,58"}}],"payload":{"tag":"h3","lines":"54,55"}}],"payload":{"tag":"h2","lines":"6,7"}},{"colorFreezeLevel":2})</script>
</body>
</html>
